{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hxxzhang/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-10-01 21:24:15.393820: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-01 21:24:15.406879: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-01 21:24:15.419506: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-01 21:24:15.423359: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-01 21:24:15.435317: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-01 21:24:16.739447: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Import lib\n",
    "import torch\n",
    "from transformers import AutoTokenizer, OPTForCausalLM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List\n",
    "import json\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import argparse,os\n",
    "import csv\n",
    "from diffusers import UNet2DModel, AutoPipelineForText2Image, UNet2DModel\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, PNDMScheduler\n",
    "from PIL import Image\n",
    "from diffusers import LMSDiscreteScheduler\n",
    "from tqdm.auto import tqdm\n",
    "from torch import autocast\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hxxzhang/miniconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "CLIPModel is using CLIPSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    }
   ],
   "source": [
    "# Embedding SFW and NSFW\n",
    "max_len =0\n",
    "token_size = 768 # stable diffsuion 1.4\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "torch_device = \"cuda\"\n",
    "text_encoder = text_encoder.to(torch_device)\n",
    "\n",
    "# SAFE PROMPT and UNSAFE phrases\n",
    "sfw_prompts=open(\"YOUR PATH TO SFW PROMPTs\").readlines()\n",
    "nsfw = open(\"YOUR PATH TO NSFW PROMPTs\").readlines()\n",
    "\n",
    "sfw_vec = []\n",
    "nsfw_vec = []\n",
    "\n",
    "num,cnt=-1,0\n",
    "winsize = 9\n",
    "\n",
    "# Sub-components of text embeddings from safe prompts are also considered safe\n",
    "for sfw_prompt in sfw_prompts:\n",
    "    num+=1\n",
    "    tokenized_sfw = tokenizer(sfw_prompt,truncation=True,return_tensors=\"pt\")\n",
    "    encode_sfw = text_encoder(tokenized_sfw.input_ids.to(torch_device),output_attentions=True,output_hidden_states=True)\n",
    "    flag=0\n",
    "    for i in range(1,encode_sfw.last_hidden_state.shape[1]-1):\n",
    "        for j in range(1,winsize+1):\n",
    "            tail=min(encode_sfw.last_hidden_state.shape[1]-1,i+j)\n",
    "            tmp = encode_sfw.last_hidden_state[0][i:tail].cpu().detach().numpy()\n",
    "            # add padding\n",
    "            vec = np.zeros((9, token_size), dtype=np.float32)\n",
    "            vec[0:tail-i]=tmp\n",
    "            sfw_vec.append(vec)\n",
    "\n",
    "for i in nsfw:\n",
    "    tokenized_nsfw = tokenizer(i, padding='max_length', max_length=11,return_tensors=\"pt\")\n",
    "    input_ids = tokenized_nsfw.input_ids\n",
    "    hidden_states = text_encoder(tokenized_nsfw.input_ids.to(torch_device)).last_hidden_state\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "    pad_mask = (input_ids == pad_token_id)\n",
    "    # Overwrite PAD token embeddings to be the 0\n",
    "    pad_embedding = torch.zeros(hidden_states.size(-1),dtype=hidden_states.dtype).to(torch_device)\n",
    "    hidden_states[pad_mask] = pad_embedding\n",
    "    nsfw_vec.append(hidden_states[0][1:-1].cpu().detach().numpy())\n",
    "\n",
    "sfw_embeddings = np.array(sfw_vec)\n",
    "nsfw_embeddings = np.array(nsfw_vec)\n",
    "# Filter out elements that do not have the shape (9, 768)\n",
    "nsfw_vec = [i for i in nsfw_vec if i.shape == (9, 768)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Flatten the data\n",
    "sfw_flattened = sfw_embeddings.reshape(sfw_embeddings.shape[0], -1)\n",
    "nsfw_flattened = nsfw_embeddings.reshape(nsfw_embeddings.shape[0], -1)\n",
    "\n",
    "# combined_data = np.concatenate((nsfw_flattened, sfw_flattened), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-layer perceptron classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hxxzhang/miniconda3/lib/python3.12/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "2024-08-27 23:34:40.401635: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2343] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f7770466480>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Combine the data and create labels\n",
    "X = np.concatenate((nsfw_embeddings, sfw_embeddings), axis=0)\n",
    "y = np.concatenate((np.ones(nsfw_embeddings.shape[0]), np.zeros(sfw_embeddings.shape[0])), axis=0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=42)\n",
    "\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(9, token_size)),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(50, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=30, batch_size=32, validation_split=0, verbose=0)\n",
    "\n",
    "model.save('SAVED MODEL PATH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = (y_pred_prob > 0.5).astype(\"int32\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(conf_matrix)\n",
    "\n",
    "# Visualize the Confusion Matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['SFW', 'NSFW'], yticklabels=['SFW', 'NSFW'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate identification performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hate 18 63 0.19480519480519481\n",
      "harassment 36 157 0.14684466019417475\n",
      "violence 73 209 0.17989417989417988\n",
      "self-harm 97 243 0.1822721598002497\n",
      "sexual 230 431 0.2158968850698174\n",
      "shocking 117 308 0.22313084112149534\n",
      "illegal activity 53 132 0.10866574965612105\n"
     ]
    }
   ],
   "source": [
    "# Naive black list\n",
    "import re\n",
    "category = ['hate','harassment', 'violence', 'self-harm','sexual','shocking','illegal activity']\n",
    "for cat in category:\n",
    "    prompts=pd.read_csv('/home/hxxzhang/P4D/data/unsafe-prompts4703.csv')\n",
    "    prompts = prompts[prompts['categories'].str.contains(cat, na=False)]\n",
    "    index = open('/home/hxxzhang/Gen_img/gen_img/sd/sd1.4_step50/nudenet.txt').read().splitlines()\n",
    "    index += [i.split('_')[0] for i in pd.read_csv('/home/hxxzhang/Gen_img/gen_img/sd/sd1.4_step50/inappropriate/Clip_ViT-L/sim_prompt_tuneddata/inappropriate_images.csv',header=None)[3].tolist()]\n",
    "    index =set(index)\n",
    "    nsfw_prompt= []\n",
    "    for i in prompts['label']:\n",
    "        if str(i) in index:\n",
    "            nsfw_prompt.append(prompts['prompt'][i])\n",
    "    nsfw_prompts = nsfw_prompt\n",
    "    iden = 0\n",
    "    for prompt in nsfw_prompts:\n",
    "        for i in nsfw:\n",
    "            if re.search(rf'\\b{re.escape(i.strip())}\\b', prompt):\n",
    "                iden+=1\n",
    "                break\n",
    "    print(cat,iden,len(nsfw_prompts),(len(nsfw_prompts)-iden)/len(prompts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 4246, 4702, 0.0, 0.9030199914929817)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I2P UNSAFE PROMPTS\n",
    "nsfw_prompts=pd.read_csv('/home/hxxzhang/SafeDiff/dataset/nsfw/i2p_benchmark.csv')['prompt'].tolist()\n",
    "\n",
    "num,cnt=-1,0\n",
    "allcnt=0\n",
    "winsize = 9\n",
    "# with open('/home/hxxzhang/SafeDiff/gen_img/v1.4_iden/miss2.txt','w') as file:\n",
    "for nsfw_prompt in nsfw_prompts:\n",
    "    num+=1\n",
    "    tokenized_nsfw = tokenizer(nsfw_prompt,truncation=True,return_tensors=\"pt\")\n",
    "    encode_nsfw = text_encoder(tokenized_nsfw.input_ids.to(torch_device),output_attentions=True,output_hidden_states=True)\n",
    "    flag=0\n",
    "    prompt_vec_set = []\n",
    "    pos_map = {}\n",
    "    pos=0\n",
    "    for i in range(1,encode_nsfw.last_hidden_state.shape[1]-1):\n",
    "        for j in range(1,winsize+1):\n",
    "            tail=min(encode_nsfw.last_hidden_state.shape[1]-1,i+j)\n",
    "            tmp = encode_nsfw.last_hidden_state[0][i:tail].cpu().detach().numpy()\n",
    "            # add padding\n",
    "            vec = np.zeros((9, token_size), dtype=np.float32)\n",
    "            vec[0:tail-i]=tmp\n",
    "            prompt_vec_set.append(vec)\n",
    "            pos_map[pos]=[i,i+j]\n",
    "            pos+=1\n",
    "    pred_prob = model.predict(np.array(prompt_vec_set),verbose=0)\n",
    "    start_set={}\n",
    "    ans=[]\n",
    "    if [1] in (pred_prob > 0.5).astype(\"int32\"):\n",
    "        allcnt+=1\n",
    "cnt,allcnt, num, cnt/num, allcnt/num"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
