{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hxxzhang/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/hxxzhang/miniconda3/lib/python3.12/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/hxxzhang/miniconda3/lib/python3.12/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/hxxzhang/miniconda3/lib/python3.12/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/hxxzhang/miniconda3/lib/python3.12/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "2024-08-27 05:47:06.707178: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-27 05:47:06.722076: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-27 05:47:06.733919: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-27 05:47:06.738450: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-27 05:47:06.749538: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-27 05:47:07.842959: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/hxxzhang/miniconda3/lib/python3.12/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, OPTForCausalLM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List\n",
    "import json\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import argparse,os\n",
    "import csv\n",
    "from diffusers import UNet2DModel, AutoPipelineForText2Image, UNet2DModel\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, PNDMScheduler\n",
    "from PIL import Image\n",
    "from diffusers import LMSDiscreteScheduler\n",
    "from tqdm.auto import tqdm\n",
    "from torch import autocast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize safe phrases from safe prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "\n",
    "def tokenize_and_clean(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove symbols and punctuation using regex\n",
    "    tokens = [re.sub(r'\\W+', '', token) for token in tokens if re.sub(r'\\W+', '', token)]\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "    \n",
    "    # Get parts of speech tags\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    \n",
    "    # Remove adpositions (prepositions and postpositions)\n",
    "    tokens = {word for word, pos in pos_tags if pos not in ['IN']}\n",
    "    \n",
    "    return tokens\n",
    "def chunk_phrases(text):\n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # POS tagging\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    \n",
    "    # Define a chunk grammar\n",
    "    # Here we define chunk patterns for noun phrases (NP)\n",
    "    grammar = r\"\"\"\n",
    "      NP: {<DT>?<JJ>*<NN.*>}  # Chunk sequences of DT, JJ, NN\n",
    "          {<NN.*><NN.*>}      # Chunk consecutive nouns\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a chunk parser\n",
    "    chunk_parser = nltk.RegexpParser(grammar)\n",
    "    \n",
    "    # Parse the POS tagged tokens to chunk phrases\n",
    "    tree = chunk_parser.parse(pos_tags)\n",
    "    \n",
    "    # Extract and return the chunked phrases\n",
    "    chunked_phrases = set()\n",
    "    for subtree in tree:\n",
    "        if type(subtree) == nltk.Tree:\n",
    "            phrase = \" \".join([token for token, pos in subtree.leaves()])\n",
    "            chunked_phrases.add(phrase)\n",
    "    \n",
    "    return chunked_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/hxxzhang/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phrases from safe prompts can be considered safe\n",
    "sfw_prompts=pd.read_csv(\"PATH to NORMAL PROMPT\")['prompt'].tolist()\n",
    "sfw_phrase=set()\n",
    "for prompt in sfw_prompts:\n",
    "    sfw_phrase  = sfw_phrase.union(chunk_phrases(prompt))\n",
    "\n",
    "with open(\"PATH to SAVE SAFE PHRASES\",'w') as file:\n",
    "    for sfw in sfw_phrase:\n",
    "        file.write(sfw+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "torch_device = \"cuda\"\n",
    "text_encoder = text_encoder.to(torch_device)\n",
    "\n",
    "for i in sfw_phrase:\n",
    "    tokenized_sfw = tokenizer(i,truncation=True,return_tensors=\"pt\")\n",
    "    encode_sfw = text_encoder(tokenized_sfw.input_ids.to(torch_device))\n",
    "    max_len=max(max_len,encode_sfw.last_hidden_state.shape[1])\n",
    "print(\"MAX LEN of text embedding is:\",max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate SFW phrases with NSFW phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='No bad content detected.', refusal=None, role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "# Utilize gpt-4o to generate SFW phrases with given NSFW phrases\n",
    "from openai import OpenAI\n",
    "OPENAI_API_KEY = \"OPENAI API KEY\"\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "ans=[]\n",
    "for prompt in nsfw:\n",
    "    completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a NSFW and SFW safety assistant. Return the safe and benign opposite meaning phrase of the given phrase. If the safe and benign opposite meaning phrase does not exist, then use the more general and benign word. Only return the phrase no other words, no other response.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    )\n",
    "    ans.append(completion.choices[0].message)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
